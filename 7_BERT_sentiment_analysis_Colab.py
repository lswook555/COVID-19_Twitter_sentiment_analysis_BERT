# -*- coding: utf-8 -*-
"""BERT_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14MffSrMg1s5V2_S3PH4eDazQ_dzcgKAN
"""

import tensorflow as tf
!pip install transformers

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model.summary()

import pandas as pd
#import dataset
df = pd.read_csv("/content/preprecessed_training_test_data_01132021.csv")
df.head(10)
len(df)

# data processing to make the data fit in BERT model
df = df.dropna()

# Anger data
df_anger = df.iloc[:,[1,3]]
df_anger.columns = ['DATA_COLUMN','LABEL_COLUMN']

# Splitting training data file into *train* and *dev*
from sklearn.model_selection import train_test_split
df_anger_train1, df_anger_test = train_test_split(df_anger, 
                                                 test_size=0.20,
                                                 random_state = 555)

df_anger_train2, df_anger_dev = train_test_split(df_anger_train1, 
                                                 test_size=0.20,
                                                 random_state = 555)


len(df_anger_train2)

len(df_anger_dev)

len(df_anger_test)
df_anger_test.head

# functions 
# convert_data_to_examples
# convert_examples_to_tf_dataset
def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): 
  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)

  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)
  
  return train_InputExamples, validation_InputExamples

  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, 
                                                                           test, 
                                                                           'DATA_COLUMN', 
                                                                           'LABEL_COLUMN')
  
def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):
    features = [] # -> will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text_a,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) > max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length
            truncation=True
        )

        input_ids, token_type_ids, attention_mask = (input_dict["input_ids"],
            input_dict["token_type_ids"], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    "input_ids": f.input_ids,
                    "attention_mask": f.attention_mask,
                    "token_type_ids": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({"input_ids": tf.int32, "attention_mask": tf.int32, "token_type_ids": tf.int32}, tf.int64),
        (
            {
                "input_ids": tf.TensorShape([None]),
                "attention_mask": tf.TensorShape([None]),
                "token_type_ids": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )


DATA_COLUMN = 'DATA_COLUMN'
LABEL_COLUMN = 'LABEL_COLUMN'

# apply functions
train_InputExamples, validation_InputExamples = convert_data_to_examples(df_anger_train2, df_anger_dev, DATA_COLUMN, LABEL_COLUMN)

train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)
train_data = train_data.shuffle(100).batch(40).repeat(2)

validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)
validation_data = validation_data.batch(40)

# Configuring the BERT model and Fine-tuning
with tf.device('/gpu:0'):
  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

  model.fit(train_data, epochs=1, validation_data=validation_data)

pred_sentences = df_anger_test['DATA_COLUMN'].tolist()

tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')
tf_outputs = model(tf_batch)
tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)
labels = [0,1]
label = tf.argmax(tf_predictions, axis=1)
label = label.numpy()
output = []
for i in range(len(pred_sentences)):
  output.append(labels[label[i]])

df_anger_test['PREDICTED_LABEL'] = output

df_anger_test.head(10)

from sklearn.metrics import confusion_matrix
confusion_matrix(df_anger_test['LABEL_COLUMN'],df_anger_test['PREDICTED_LABEL'])

from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(df_anger_test['LABEL_COLUMN'],df_anger_test['PREDICTED_LABEL'],average=None)

from sklearn.metrics import accuracy_score
 accuracy_score(df_anger_test['LABEL_COLUMN'],df_anger_test['PREDICTED_LABEL'])

# saving model
from transformers import BertForSequenceClassification
model.save_pretrained('./COVID_BERT_V1/')
# pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)